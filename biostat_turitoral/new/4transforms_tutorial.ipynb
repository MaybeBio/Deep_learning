{"cells":[{"cell_type":"markdown","metadata":{"id":"s3hIkXPxvgfo"},"source":["[Learn the Basics](intro.html) \\|\\|\n","[Quickstart](quickstart_tutorial.html) \\|\\|\n","[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n","DataLoaders](data_tutorial.html) \\|\\| **Transforms** \\|\\| [Build\n","Model](buildmodel_tutorial.html) \\|\\|\n","[Autograd](autogradqs_tutorial.html) \\|\\|\n","[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n","Model](saveloadrun_tutorial.html)\n","\n","Transforms\n","==========\n","\n","Data does not always come in its final processed form that is required\n","for training machine learning algorithms. We use **transforms** to\n","perform some manipulation of the data and make it suitable for training.\n","\n","All TorchVision datasets have two parameters -`transform` to modify the\n","features and `target_transform` to modify the labels - that accept\n","callables containing the transformation logic. The\n","[torchvision.transforms](https://pytorch.org/vision/stable/transforms.html)\n","module offers several commonly-used transforms out of the box.\n","\n","The FashionMNIST features are in PIL Image format, and the labels are\n","integers. For training, we need the features as normalized tensors, and\n","the labels as one-hot encoded tensors. To make these transformations, we\n","use `ToTensor` and `Lambda`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gI771JHqvgfq","executionInfo":{"status":"ok","timestamp":1746537580841,"user_tz":-480,"elapsed":33210,"user":{"displayName":"Feng Xue","userId":"00196250428882015334"}},"outputId":"f204e705-f025-48e9-bada-24885175b7be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26.4M/26.4M [00:02<00:00, 12.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29.5k/29.5k [00:00<00:00, 198kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.42M/4.42M [00:01<00:00, 3.69MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5.15k/5.15k [00:00<00:00, 8.39MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor, Lambda\n","\n","ds = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor(),\n","    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",")"]},{"cell_type":"markdown","metadata":{"id":"LquqvOnYvgfq"},"source":["ToTensor()\n","==========\n","\n","[ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor)\n","converts a PIL image or NumPy `ndarray` into a `FloatTensor`. and scales\n","the image\\'s pixel intensity values in the range \\[0., 1.\\]\n"]},{"cell_type":"markdown","metadata":{"id":"4lzbKQ9Xvgfq"},"source":["Lambda Transforms\n","=================\n","\n","Lambda transforms apply any user-defined lambda function. Here, we\n","define a function to turn the integer into a one-hot encoded tensor. It\n","first creates a zero tensor of size 10 (the number of labels in our\n","dataset) and calls\n","[scatter\\_](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html)\n","which assigns a `value=1` on the index as given by the label `y`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TScCZtUQvgfr"},"outputs":[],"source":["target_transform = Lambda(lambda y: torch.zeros(\n","    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"]},{"cell_type":"markdown","metadata":{"id":"IBp34rgevgfr"},"source":["------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"BLSqcMkKvgfs"},"source":["Further Reading\n","===============\n","\n","-   [torchvision.transforms\n","    API](https://pytorch.org/vision/stable/transforms.html)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}