{"cells":[{"cell_type":"markdown","metadata":{"id":"ddptozjXw47a"},"source":["[Learn the Basics](intro.html) \\|\\|\n","[Quickstart](quickstart_tutorial.html) \\|\\|\n","[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n","DataLoaders](data_tutorial.html) \\|\\|\n","[Transforms](transforms_tutorial.html) \\|\\| **Build Model** \\|\\|\n","[Autograd](autogradqs_tutorial.html) \\|\\|\n","[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n","Model](saveloadrun_tutorial.html)\n","\n","Build the Neural Network\n","========================\n","\n","Neural networks comprise of layers/modules that perform operations on\n","data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace\n","provides all the building blocks you need to build your own neural\n","network. Every module in PyTorch subclasses the\n","[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n","A neural network is a module itself that consists of other modules\n","(layers). This nested structure allows for building and managing complex\n","architectures easily.\n","\n","In the following sections, we\\'ll build a neural network to classify\n","images in the FashionMNIST dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wv36b6YNw47c"},"outputs":[],"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"]},{"cell_type":"markdown","metadata":{"id":"Lx1ANdDDw47c"},"source":["Get Device for Training\n","=======================\n","\n","We want to be able to train our model on an\n","[accelerator](https://pytorch.org/docs/stable/torch.html#accelerators)\n","such as CUDA, MPS, MTIA, or XPU. If the current accelerator is\n","available, we will use it. Otherwise, we use the CPU.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bea6HOOOw47d","executionInfo":{"status":"ok","timestamp":1746537868681,"user_tz":-480,"elapsed":16,"user":{"displayName":"Feng Xue","userId":"00196250428882015334"}},"outputId":"4ce7e369-3332-44f2-d0a3-56daf7116f80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n"]}],"source":["device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","metadata":{"id":"Fi71EB3Cw47d"},"source":["Define the Class\n","================\n","\n","We define our neural network by subclassing `nn.Module`, and initialize\n","the neural network layers in `__init__`. Every `nn.Module` subclass\n","implements the operations on input data in the `forward` method.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWrgyUtqw47e"},"outputs":[],"source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"1wTYL0-7w47e"},"source":["We create an instance of `NeuralNetwork`, and move it to the `device`,\n","and print its structure.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ws62AwMkw47e","executionInfo":{"status":"ok","timestamp":1746537961051,"user_tz":-480,"elapsed":21,"user":{"displayName":"Feng Xue","userId":"00196250428882015334"}},"outputId":"b7cbdb8d-2d1d-4dfe-82ce-fb47eeda3708"},"outputs":[{"output_type":"stream","name":"stdout","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}],"source":["model = NeuralNetwork().to(device)\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"ltmn0BOiw47f"},"source":["To use the model, we pass it the input data. This executes the model\\'s\n","`forward`, along with some [background\n","operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n","Do not call `model.forward()` directly!\n","\n","Calling the model on the input returns a 2-dimensional tensor with dim=0\n","corresponding to each output of 10 raw predicted values for each class,\n","and dim=1 corresponding to the individual values of each output. We get\n","the prediction probabilities by passing it through an instance of the\n","`nn.Softmax` module.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0PDb-DLw47f","executionInfo":{"status":"ok","timestamp":1746538016995,"user_tz":-480,"elapsed":47,"user":{"displayName":"Feng Xue","userId":"00196250428882015334"}},"outputId":"7555a659-0ceb-474c-e62d-89a255e95598"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: tensor([2])\n"]}],"source":["X = torch.rand(1, 28, 28, device=device)\n","logits = model(X)\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(f\"Predicted class: {y_pred}\")"]},{"cell_type":"markdown","metadata":{"id":"vZj-fbsRw47f"},"source":["------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"FuxTYG3mw47f"},"source":["Model Layers\n","============\n","\n","Let\\'s break down the layers in the FashionMNIST model. To illustrate\n","it, we will take a sample minibatch of 3 images of size 28x28 and see\n","what happens to it as we pass it through the network.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GPaKMn43w47g","executionInfo":{"status":"ok","timestamp":1746538095250,"user_tz":-480,"elapsed":10,"user":{"displayName":"Feng Xue","userId":"00196250428882015334"}},"outputId":"832acd54-14ba-4c77-c5b4-30bef6116ef7"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 28, 28])\n"]}],"source":["input_image = torch.rand(3,28,28)\n","print(input_image.size())"]},{"cell_type":"markdown","metadata":{"id":"pAydle_Jw47g"},"source":["nn.Flatten\n","==========\n","\n","We initialize the\n","[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n","layer to convert each 2D 28x28 image into a contiguous array of 784\n","pixel values ( the minibatch dimension (at dim=0) is maintained).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zIJA9foPw47g","executionInfo":{"status":"ok","timestamp":1746538137736,"user_tz":-480,"elapsed":45,"user":{"displayName":"Feng Xue","userId":"00196250428882015334"}},"outputId":"de2ea220-68f4-4a3e-f715-a07fd0d4524d"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 784])\n"]}],"source":["flatten = nn.Flatten()\n","flat_image = flatten(input_image)\n","print(flat_image.size())"]},{"cell_type":"markdown","metadata":{"id":"AYPjuO-qw47g"},"source":["nn.Linear\n","=========\n","\n","The [linear\n","layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n","is a module that applies a linear transformation on the input using its\n","stored weights and biases.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_AFGN0Qw47g","executionInfo":{"status":"ok","timestamp":1746538189546,"user_tz":-480,"elapsed":51,"user":{"displayName":"Feng Xue","userId":"00196250428882015334"}},"outputId":"d475067a-488c-4304-fb3d-5527f92ef1bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 20])\n"]}],"source":["layer1 = nn.Linear(in_features=28*28, out_features=20)\n","hidden1 = layer1(flat_image)\n","print(hidden1.size())"]},{"cell_type":"markdown","metadata":{"id":"eB-cOsA9w47h"},"source":["nn.ReLU\n","=======\n","\n","Non-linear activations are what create the complex mappings between the\n","model\\'s inputs and outputs. They are applied after linear\n","transformations to introduce *nonlinearity*, helping neural networks\n","learn a wide variety of phenomena.\n","\n","In this model, we use\n","[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n","between our linear layers, but there\\'s other activations to introduce\n","non-linearity in your model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G99MBP83w47h","executionInfo":{"status":"ok","timestamp":1746538216444,"user_tz":-480,"elapsed":40,"user":{"displayName":"Feng Xue","userId":"00196250428882015334"}},"outputId":"688ca895-6dd5-40c8-f6f8-ad992f37b864"},"outputs":[{"output_type":"stream","name":"stdout","text":["Before ReLU: tensor([[-0.8155,  0.4364,  0.1662,  0.4308, -0.4632, -0.0608,  0.4676,  0.3582,\n","          0.1318, -0.3257, -0.0410,  0.1791, -0.2960,  0.0350,  0.2795,  0.1487,\n","          0.2711,  0.0381,  0.1622,  0.7398],\n","        [-0.7836,  0.1588, -0.0540,  0.0256,  0.0607,  0.0414,  0.0875,  0.5841,\n","         -0.1266, -0.3181, -0.1275,  0.2162, -0.0972, -0.1950, -0.1328,  0.1250,\n","          0.0096,  0.2308,  0.0622,  0.4636],\n","        [-0.6345,  0.4286, -0.1851, -0.1316, -0.4850,  0.1012,  0.1033,  0.4850,\n","         -0.1706, -0.4910,  0.0943,  0.4303,  0.0551, -0.5008,  0.1604, -0.0431,\n","          0.2507, -0.2065,  0.2412,  0.3248]], grad_fn=<AddmmBackward0>)\n","\n","\n","After ReLU: tensor([[0.0000, 0.4364, 0.1662, 0.4308, 0.0000, 0.0000, 0.4676, 0.3582, 0.1318,\n","         0.0000, 0.0000, 0.1791, 0.0000, 0.0350, 0.2795, 0.1487, 0.2711, 0.0381,\n","         0.1622, 0.7398],\n","        [0.0000, 0.1588, 0.0000, 0.0256, 0.0607, 0.0414, 0.0875, 0.5841, 0.0000,\n","         0.0000, 0.0000, 0.2162, 0.0000, 0.0000, 0.0000, 0.1250, 0.0096, 0.2308,\n","         0.0622, 0.4636],\n","        [0.0000, 0.4286, 0.0000, 0.0000, 0.0000, 0.1012, 0.1033, 0.4850, 0.0000,\n","         0.0000, 0.0943, 0.4303, 0.0551, 0.0000, 0.1604, 0.0000, 0.2507, 0.0000,\n","         0.2412, 0.3248]], grad_fn=<ReluBackward0>)\n"]}],"source":["print(f\"Before ReLU: {hidden1}\\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU: {hidden1}\")"]},{"cell_type":"markdown","metadata":{"id":"79NOghVCw47h"},"source":["nn.Sequential\n","=============\n","\n","[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n","is an ordered container of modules. The data is passed through all the\n","modules in the same order as defined. You can use sequential containers\n","to put together a quick network like `seq_modules`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u79bfQhww47h"},"outputs":[],"source":["seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","input_image = torch.rand(3,28,28)\n","logits = seq_modules(input_image)"]},{"cell_type":"markdown","metadata":{"id":"w6Y46wN7w47h"},"source":["nn.Softmax\n","==========\n","\n","The last linear layer of the neural network returns [logits]{.title-ref}\n","- raw values in \\[-infty, infty\\] - which are passed to the\n","[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n","module. The logits are scaled to values \\[0, 1\\] representing the\n","model\\'s predicted probabilities for each class. `dim` parameter\n","indicates the dimension along which the values must sum to 1.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vLJnmhOQw47h"},"outputs":[],"source":["softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)"]},{"cell_type":"markdown","metadata":{"id":"qXXOQMhuw47h"},"source":["Model Parameters\n","================\n","\n","Many layers inside a neural network are *parameterized*, i.e. have\n","associated weights and biases that are optimized during training.\n","Subclassing `nn.Module` automatically tracks all fields defined inside\n","your model object, and makes all parameters accessible using your\n","model\\'s `parameters()` or `named_parameters()` methods.\n","\n","In this example, we iterate over each parameter, and print its size and\n","a preview of its values.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pcqrXU9-w47h","executionInfo":{"status":"ok","timestamp":1746538350988,"user_tz":-480,"elapsed":21,"user":{"displayName":"Feng Xue","userId":"00196250428882015334"}},"outputId":"7c924b8b-025b-46d9-a0a9-f6e3cb5a605b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure: NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n","\n","\n","Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0300, -0.0243,  0.0026,  ...,  0.0312, -0.0254, -0.0236],\n","        [ 0.0042,  0.0333,  0.0111,  ...,  0.0274,  0.0270, -0.0345]],\n","       grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0176,  0.0231], grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0435,  0.0431, -0.0073,  ...,  0.0099,  0.0136,  0.0399],\n","        [ 0.0043, -0.0251,  0.0172,  ...,  0.0379, -0.0058, -0.0275]],\n","       grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0283,  0.0142], grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0216, -0.0247,  0.0194,  ...,  0.0082, -0.0126, -0.0410],\n","        [-0.0013,  0.0440, -0.0250,  ...,  0.0138,  0.0370,  0.0360]],\n","       grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0124, -0.0219], grad_fn=<SliceBackward0>) \n","\n"]}],"source":["print(f\"Model structure: {model}\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"68dnVlsXw47i"},"source":["------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"RNJLbwBuw47i"},"source":["Further Reading\n","===============\n","\n","-   [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}